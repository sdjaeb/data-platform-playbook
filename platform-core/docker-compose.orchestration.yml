# Airflow common settings (used as an anchor for reuse across all Airflow services)
x-airflow-common: &airflow-common
  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:3.0.2} # Use Airflow 3.0.2 as specified in todo.txt
  environment: &airflow-common-env
    # Core Airflow configurations
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__CORE__AUTH_MANAGER: airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow # Connects to the 'postgres' service
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0 # Connects to the 'redis' service
    AIRFLOW__CORE__FERNET_KEY: '' # Should be set via secret in production
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'true'
    AIRFLOW__CORE__EXECUTION_API_SERVER_URL: 'http://airflow-apiserver:8080/execution/'
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
    AIRFLOW_CONN_INSURANCE_GENERATOR_API: 'http://insurance-generator:5001'
    AIRFLOW_CONN_FINANCIAL_GENERATOR_API: 'http://financial-generator:5002'
    AIRFLOW_CONN_SPORTS_GENERATOR_API: 'http://sports-generator:5003'
    # Additional Python packages for Airflow
    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}
    AIRFLOW_CONFIG: '/opt/airflow/config/airflow.cfg'
  volumes:
    - dags:/opt/airflow/dags # Mount DAGs from host
    - airflow_logs:/opt/airflow/logs # Mount logs from host
    - ./config:/opt/airflow/config # Mount Airflow config from host
    - plugins:/opt/airflow/plugins # Mount plugins from host
  user: "${AIRFLOW_UID:-50000}:0" # Set user to prevent permission issues
  networks:
    - data_platform_network

services:
  #####################
  # Airflow (Celery)  #
  #####################

  # Airflow Init: A one-off service that initializes the Airflow database,
  # creates the admin user, and sets file permissions. It runs and exits.
  airflow-init:
    <<: *airflow-common # Inherit common Airflow settings
    container_name: airflow-init
    entrypoint: /bin/bash # Override entrypoint for initial setup
    command:
      - -c
      - |
        echo "Waiting for PostgreSQL and Redis to be ready..."
        until pg_isready -h postgres -p 5432 -U airflow; do sleep 1; done
        python -c "import time, redis; r = redis.Redis(host='redis', port=6379, password=open('/run/secrets/redis_pass').read().strip()); \
          while True: \
              try: r.ping(); break \
              except Exception: time.sleep(1)"
                  echo "Running Airflow DB migration..."
                  airflow db migrate

                  echo "Creating Airflow admin user..."
                  airflow users create --username admin --password admin --firstname Airflow --lastname Admin --role Admin --email admin@example.com || true

                  echo "Setting permissions on Airflow volumes..."
                  chown -R \"${AIRFLOW_UID:-50000}:0\" /opt/airflow/
                  chown -R \"${AIRFLOW_UID:-50000}:0\" /opt/airflow/{logs,dags,plugins,config}

                  echo "Airflow initialization complete."
    user: "0:0" # Run as root for initial permissions
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    secrets:
      - postgres_user
      - postgres_pass 
      - redis_pass
      - airflow_db_url
      - redis_url
      - airflow_fernet
    networks:
      - data_platform_network

  # Airflow Webserver: The user interface for Airflow.
  airflow-webserver:
    <<: *airflow-common
    container_name: airflow-webserver
    command: api-server
    ports:
      - "8080:8080"
    environment:
      <<: *airflow-common-env
      AIRFLOW__WEBSERVER__RBAC: 'True'
      AIRFLOW__WEBSERVER__AUTHENTICATE: 'True'
      AIRFLOW__WEBSERVER__BASE_URL: 'http://localhost:8080'
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    secrets:
      - postgres_user
      - postgres_pass
      - redis_pass
      - airflow_db_url
      - redis_url
      - airflow_fernet
    networks:
      - data_platform_network
      - observability_network
    healthcheck:
      test: ["CMD", "curl", "--silent", "--fail", "http://localhost:8080/api/v2/version"]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: '1g'
 
  # Airflow Scheduler: The core component that monitors all DAGs and tasks,
  # and triggers task instances whose dependencies have been met.
  airflow-scheduler:
    <<: *airflow-common
    container_name: airflow-scheduler
    command: scheduler
    environment:
      <<: *airflow-common-env
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    secrets:
      - postgres_user
      - postgres_pass
      - redis_pass
      - airflow_db_url
      - redis_url
      - airflow_fernet
    networks:
      - data_platform_network
      - observability_network
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type SchedulerJob --hostname "$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: '1g'
 
  # Airflow Worker: Executes the tasks assigned by the Celery message broker (Redis).
  # You can scale these workers to handle more concurrent tasks.
  airflow-worker:
    <<: *airflow-common
    container_name: airflow-worker
    command: celery worker
    environment:
      <<: *airflow-common-env
      DUMB_INIT_SETSID: "0" # Required for graceful shutdown
    depends_on:
      airflow-scheduler:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    secrets:
      - postgres_user
      - postgres_pass
      - redis_pass
      - airflow_db_url
      - redis_url
      - airflow_fernet
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: '1g'
    networks: 
      - data_platform_network
      - observability_network

  # Airflow DAG Processor: Parses DAG files from the dags folder and updates them in the database.
  airflow-dag-processor:
    <<: *airflow-common
    container_name: airflow-dag-processor
    command: dag-processor
    environment:
      <<: *airflow-common-env
    depends_on:
      airflow-scheduler:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    secrets:
      - airflow_db_url
      - redis_url
      - airflow_fernet
      - postgres_user
      - postgres_pass
    networks:
      - data_platform_network
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type DagProcessorJob --hostname \"$${HOSTNAME}\" || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: '768m'
 
  # Airflow Triggerer: A separate process for running deferred tasks,
  # which is more efficient for long-running, asynchronous operations.
  airflow-triggerer:
    <<: *airflow-common
    container_name: airflow-triggerer
    command: triggerer
    environment:
      <<: *airflow-common-env
    depends_on:
      airflow-scheduler:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    secrets:
      - airflow_db_url
      - redis_url
      - airflow_fernet
      - postgres_user
      - postgres_pass
    networks:
      - data_platform_network
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type TriggererJob --hostname \"$${HOSTNAME}\" || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: '512m'