# Description: Docker Compose file for integration testing.
# Source: Deep-Dive Addendum: IaC & CI/CD Recipes, Section 5.4.
#
# This file defines a stripped-down set of services specifically for
# integration testing, focusing on inter-service communication.
version: '3.8'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
    healthcheck:
      test: ["CMD", "sh", "-c", "nc -z localhost 2181"]
      interval: 10s
      timeout: 5s
      retries: 5
  kafka:
    image: confluentinc/cp-kafka:7.4.0
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    healthcheck:
      test: ["CMD", "sh", "-c", "kafka-topics --bootstrap-server localhost:9092 --list"]
      interval: 10s
      timeout: 5s
      retries: 5
  minio:
    image: minio/minio:latest
    ports:
      - "9000:9000"
    environment:
      MINIO_ROOT_USER: test_user
      MINIO_ROOT_PASSWORD: test_password
    command: server /data --console-address ":9000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
  fastapi_ingestor:
    build: ./fastapi_app
    environment:
      KAFKA_BROKER: kafka:29092
      KAFKA_TOPIC: raw_data_test
    depends_on:
      kafka:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health || exit 1"]
      interval: 5s
      timeout: 3s
      retries: 5
  # Spark service for integration testing (can be a standalone driver in test, or a small cluster)
  spark-test-runner:
    image: bitnami/spark:3.5.0
    depends_on:
      kafka:
        condition: service_healthy
      minio:
        condition: service_healthy
    environment:
      SPARK_MASTER_URL: "local[*]" # Run Spark in local mode for test
      KAFKA_BROKER: kafka:29092
      MINIO_HOST: minio
      MINIO_ACCESS_KEY: test_user
      MINIO_SECRET_KEY: test_password
    volumes:
      - ./pyspark_jobs:/opt/bitnami/spark/data/pyspark_jobs # Mount jobs
      - ./data/test_spark_output:/tmp/spark_output # Output dir for tests
    # No exposed ports unless needed for Spark UI inspection during debug
    command: ["tail", "-f", "/dev/null"] # Keep container running