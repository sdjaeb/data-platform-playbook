version: '3.8'
services:
  postgres:
    image: postgres:15
    container_name: starter-postgres
    restart: unless-stopped
    environment:
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
      POSTGRES_DB: starter_db
    volumes:
      - ./data/starter-postgres:/var/lib/postgresql/data
    ports:
      - "5432:5432" # Exposed for direct access and FastAPI connectivity
  minio:
    image: minio/minio:latest
    container_name: starter-minio
    restart: unless-stopped
    ports:
      - "9000:9000" # MinIO API port
      - "9901:9001" # MinIO Console UI port
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    volumes:
      - ./data/starter-minio:/data # Persistent volume for MinIO data
    command: server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: intermediate-zookeeper
    restart: unless-stopped
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: intermediate-kafka
    restart: unless-stopped
    depends_on:
      - zookeeper
    ports:
      - "9092:9092" # Expose Kafka broker port for external access
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
  fastapi_ingestor:
    build: ./fastapi_app
    container_name: intermediate-fastapi-ingestor
    restart: unless-stopped
    ports:
      - "8000:8000"
    environment:
      KAFKA_BROKER: kafka:29092 # Important: use Kafka service name for internal Docker communication
      KAFKA_TOPIC: raw_financial_insurance_data
    volumes:
      - ./src/fastapi_app_intermediate:/app/app # Updated ingestor to publish to Kafka
    depends_on:
      kafka:
        condition: service_healthy # Ensure Kafka is healthy before FastAPI tries to connect
  spark-master:
    image: bitnami/spark:3.5.0
    container_name: intermediate-spark-master
    restart: unless-stopped
    command: /opt/bitnami/spark/bin/spark-shell # Or spark-class org.apache.spark.deploy.master.Master
    environment:
      SPARK_MODE: master
      SPARK_RPC_AUTHENTICATION_ENABLED: "no"
      SPARK_EVENT_LOG_ENABLED: "true"
      SPARK_EVENT_LOG_DIR: "/opt/bitnami/spark/events"
      SPARK_SUBMIT_ARGS: --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,io.delta:delta-core_2.12:2.4.0 --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog"
    ports:
      - "8080:8080" # Spark Master UI
      - "7077:7077" # Spark Master internal communication
    volumes:
      - ./data/spark-events:/opt/bitnami/spark/events # For Spark History Server
      - ./pyspark_jobs:/opt/bitnami/spark/data/pyspark_jobs # Mount PySpark jobs
  spark-worker-1:
    image: bitnami/spark:3.5.0
    container_name: intermediate-spark-worker-1
    restart: unless-stopped
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_CORES: 1
      SPARK_WORKER_MEMORY: 1G
      SPARK_EVENT_LOG_ENABLED: "true"
      SPARK_EVENT_LOG_DIR: "/opt/bitnami/spark/events"
      SPARK_SUBMIT_ARGS: --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,io.delta:delta-core_2.12:2.4.0 --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog"
    volumes:
      - ./data/spark-events:/opt/bitnami/spark/events
    depends_on:
      spark-master:
        condition: service_healthy
      kafka:
        condition: service_healthy # Dependency on Kafka
      minio:
        condition: service_healthy # Dependency on MinIO

  # Airflow Services
  airflow-scheduler:
    image: apache/airflow:2.8.1
    container_name: advanced-airflow-scheduler
    restart: always
    depends_on:
      airflow-webserver:
        condition: service_healthy
      postgres: # Airflow metadata database
        condition: service_healthy
      kafka: # For DAGs that interact with Kafka (e.g., Spark jobs)
        condition: service_healthy
    environment:
      AIRFLOW_HOME: /opt/airflow
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
      AIRFLOW__CORE__EXECUTOR: LocalExecutor # For local dev;
      CeleryExecutor for production
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://user:password@postgres/main_db
      AIRFLOW__WEBSERVER__WEB_SERVER_PORT: 8080
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    volumes:
      - ./airflow_dags:/opt/airflow/dags
      - ./data/airflow_logs:/opt/airflow/logs
      - ./orchestrator/plugins:/opt/airflow/plugins # If you have custom plugins
    command: scheduler
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname $$HOSTNAME"]
      interval: 10s
      timeout: 10s
      retries: 5
  airflow-webserver:
    image: apache/airflow:2.8.1
    container_name: advanced-airflow-webserver
    restart: always
    depends_on:
      postgres:
        condition: service_healthy
    ports:
      - "8081:8080" # Mapped to 8081 to avoid conflict with Spark Master UI
    environment:
      AIRFLOW_HOME: /opt/airflow
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://user:password@postgres/main_db
      AIRFLOW__WEBSERVER__WEB_SERVER_PORT: 8080
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    volumes:
      - ./airflow_dags:/opt/airflow/dags
      - ./data/airflow_logs:/opt/airflow/logs
      - ./orchestrator/plugins:/opt/airflow/plugins
    command: webserver
    healthcheck:
      test: ["CMD-SHELL", "curl --silent --fail http://localhost:8080/health"]
      interval: 10s
      timeout: 10s
      retries: 5
  # Observability Components
  grafana:
    image: grafana/grafana:latest
    container_name: advanced-grafana
    restart: unless-stopped
    ports:
      - "3000:3000" # Grafana Web UI
    volumes:
      - ./data/grafana:/var/lib/grafana # Persistent storage for Grafana data
      - ./observability/grafana_dashboards:/etc/grafana/provisioning/dashboards # Mount dashboards
      - ./observability/grafana_datasources:/etc/grafana/provisioning/datasources # Mount datasources
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: admin
    depends_on:
      grafana_alloy:
        condition: service_started
      cadvisor:
        condition: service_started
  grafana_alloy:
    image: grafana/alloy:latest
    container_name: advanced-grafana_alloy
    restart: unless-stopped
    ports:
      - "4317:4317" # OTLP gRPC endpoint for receiving telemetry
      - "4318:4318" # OTLP HTTP endpoint for receiving telemetry
      - "12345:12345" # Example Prometheus scrape port for Grafana to pull metrics from Alloy
    volumes:
      - ./observability/alloy-config.river:/etc/alloy/config.river # Mount your Alloy configuration
    command: -config.file=/etc/alloy/config.river
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.47.0 # Stable version for container metrics
    container_name: advanced-cadvisor
    restart: unless-stopped
    ports:
      - "8082:8080" # Default cAdvisor UI/metrics port (mapped to 8082 to avoid conflicts)
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:rw
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    command: --listen_ip=0.0.0.0 --port=8080 # Expose on all interfaces on port 8080
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:8080/metrics || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
  # Data Lineage (Spline) Components
  spline-rest:
    image: aballon/spline-rest-server:latest # Use a specific version, e.g., 0.7.1
    container_name: advanced-spline-rest
    restart: unless-stopped
    ports:
      - "8083:8080" # Spline REST API server (mapped to 8083 to avoid conflicts)
    depends_on:
      postgres: # Spline can use a persistent DB for metadata
        condition: service_healthy
  spline-ui:
    image: aballon/spline-web-ui:latest # Use a specific version, e.g., 0.7.1
    container_name: advanced-spline-ui
    restart: unless-stopped
    ports:
      - "9090:80" # Spline Web UI
    environment:
      SPLINE_API_URL: http://spline-rest:8080 # Connects to the spline-rest service
    depends_on:
      - spline-rest
  # Metadata Management (OpenMetadata) Components
  openmetadata-mysql:
    image: mysql:8.0
    container_name: advanced-openmetadata-mysql
    restart: unless-stopped
    environment:
      MYSQL_ROOT_PASSWORD: openmetadata_user
      MYSQL_USER: openmetadata_user
      MYSQL_PASSWORD: openmetadata_password
      MYSQL_DATABASE: openmetadata_db
    volumes:
      - ./data/openmetadata_mysql:/var/lib/mysql
    ports:
      - "3306:3306"
    command: --default-authentication-plugin=mysql_native_password
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost", "-u$$MYSQL_USER", "-p$$MYSQL_PASSWORD"]
      interval: 10s
      timeout: 5s
      retries: 5
  openmetadata-elasticsearch:
    image: opensearchproject/opensearch:2.11.0 # Or elasticsearch:7.17.10
    container_name: advanced-openmetadata-elasticsearch
    restart: unless-stopped
    environment:
      discovery.type: single-node
      OPENSEARCH_JAVA_OPTS: "-Xms512m -Xmx512m"
    ports:
      - "9200:9200" # HTTP API
      - "9600:9600" # Transport port
    volumes:
      - ./data/openmetadata_elasticsearch:/usr/share/opensearch/data
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cat/health?h=st | grep -q green"]
      interval: 10s
      timeout: 10s
      retries: 5
  openmetadata-server:
    image: openmetadata/openmetadata:1.3.1
    container_name: advanced-openmetadata-server
    restart: unless-stopped
    depends_on:
      openmetadata-mysql:
        condition: service_healthy
      openmetadata-elasticsearch:
        condition: service_healthy
    ports:
      - "8585:8585" # OpenMetadata Web UI
    environment:
      MYSQL_HOST: openmetadata-mysql
      MYSQL_PORT: 3306
      MYSQL_DATABASE: openmetadata_db
      MYSQL_USER: openmetadata_user
      MYSQL_PASSWORD: openmetadata_password
      ELASTICSEARCH_HOST: openmetadata-elasticsearch
      ELASTICSEARCH_PORT: 9200
      APP_ENV: local
    command: ["./docker/run_server.sh"]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8585/api/v1/health | grep -q OK"]
      interval: 30s
      timeout: 20s
      retries: 5
  openmetadata-ingestion:
    image: openmetadata/ingestion-base:1.3.1
    container_name: advanced-openmetadata-ingestion
    restart: on-failure
    depends_on:
      openmetadata-server:
        condition: service_healthy
    environment:
      OPENMETADATA_SERVER_URL: http://openmetadata-server:8585
    volumes:
      - ./openmetadata_ingestion_scripts:/opt/openmetadata/examples/workflows