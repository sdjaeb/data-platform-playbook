# docker-compose.yml
# This file defines and configures the multi-service enterprise data platform
# using Docker Compose. It orchestrates services like PostgreSQL, MinIO, Kafka,
# FastAPI, Spark, Airflow, Grafana, Grafana Alloy, cAdvisor, Spline, OpenMetadata,
# and a custom MinIO Webhook Listener.

version: '3.8'

services:
  # --- Core Data Services ---
  postgres:
    image: postgres:15
    container_name: starter-postgres
    restart: unless-stopped
    environment:
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
      POSTGRES_DB: starter_db
    volumes:
      - ./data/starter-postgres:/var/lib/postgresql/data
    ports:
      - "5432:5432" # Exposed for direct access and FastAPI/Spark connectivity
    healthcheck: # Added healthcheck
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB"]
      interval: 5s
      timeout: 5s
      retries: 5

  minio:
    image: minio/minio:latest
    container_name: starter-minio
    restart: unless-stopped
    ports:
      - "9000:9000" # MinIO API port
      - "9001:9001" # MinIO Console UI port (Changed from 9901 to 9001 for consistency)
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    volumes:
      - ./data/starter-minio:/data # Persistent volume for MinIO data
    command: server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: intermediate-zookeeper
    restart: unless-stopped
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    healthcheck: # Added healthcheck
      test: ["CMD", "bash", "-c", "echo stat | nc localhost 2181"]
      interval: 10s
      timeout: 5s
      retries: 5

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: intermediate-kafka
    restart: unless-stopped
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9092:9092" # Expose Kafka broker port for external access (e.g., simulate_data.py)
      - "29092:29092" # Internal Docker network listener (for other services like FastAPI, Spark)
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    healthcheck: # Added healthcheck
      test: ["CMD-SHELL", "kafka-topics --bootstrap-server localhost:29092 --list"]
      interval: 10s
      timeout: 5s
      retries: 5

  fastapi_ingestor:
    build: ./fastapi_app # Build from local Dockerfile in fastapi_app directory
    container_name: intermediate-fastapi-ingestor
    restart: unless-stopped
    ports:
      - "8000:8000" # Expose FastAPI API port
    environment:
      KAFKA_BROKER: kafka:29092 # Use Kafka service name for internal Docker communication
      KAFKA_TOPIC_FINANCIAL: raw_financial_transactions # Specific topic for financial data
      KAFKA_TOPIC_INSURANCE: raw_insurance_claims # Specific topic for insurance data
      OTEL_EXPORTER_OTLP_ENDPOINT: http://grafana_alloy:4318 # OpenTelemetry endpoint for Alloy
    volumes:
      - ./fastapi_app/app:/app/app # Mount FastAPI app code
    depends_on:
      kafka:
        condition: service_healthy # Ensure Kafka is healthy before FastAPI
      grafana_alloy: # Dependency on Alloy for telemetry export
        condition: service_healthy
    healthcheck: # Added healthcheck
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 5

  mongodb: # Added MongoDB service
    image: mongo:latest
    container_name: starter-mongodb
    restart: unless-stopped
    ports:
      - "27017:27017" # Exposed for direct access
    environment:
      MONGO_INITDB_ROOT_USERNAME: root
      MONGO_INITDB_ROOT_PASSWORD: password
    volumes:
      - ./data/starter-mongodb:/data/db # Persistent volume for MongoDB data
    healthcheck: # Added healthcheck
      test: ["CMD", "mongosh", "--eval", "db.adminCommand('ping').ok"]
      interval: 10s
      timeout: 5s
      retries: 5

  # --- Spark Services (Configured for Spline) ---
  spark-master:
    image: bitnami/spark:3.5.0
    container_name: advanced-spark-master
    restart: unless-stopped
    command: /opt/bitnami/spark/bin/spark-class org.apache.spark.deploy.master.Master
    environment:
      SPARK_MODE: master
      SPARK_RPC_AUTHENTICATION_ENABLED: "no"
      SPARK_EVENT_LOG_ENABLED: "true"
      SPARK_EVENT_LOG_DIR: "/opt/bitnami/spark/events"
      # Global Spark Submit Args for packages and extensions.
      # Spline agent options are added dynamically via spark-submit command line in DAGs/scripts.
      SPARK_SUBMIT_ARGS: >
        --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,io.delta:delta-core_2.12:2.4.0,org.postgresql:postgresql:42.6.0,org.mongodb.spark:mongo-spark-connector_2.12:10.0.0
        --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
        --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog
        --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000
        --conf spark.hadoop.fs.s3a.access.key=minioadmin
        --conf spark.hadoop.fs.s3a.secret.key=minioadmin
        --conf spark.hadoop.fs.s3a.path.style.access=true
        --conf spark.databricks.delta.properties.defaults.enableChangeDataFeed=true
    ports:
      - "8088:8080" # Spark Master UI (Mapped to 8088 to avoid conflict with Airflow Webserver, which is 8081)
      - "7077:7077" # Spark Master internal communication
    volumes:
      - ./data/spark-events:/opt/bitnami/spark/events # For Spark History Server
      - ./pyspark_jobs:/opt/bitnami/spark/jobs # Mount PySpark jobs
      - spline_jars:/opt/bitnami/spark/jars # Mount Spline JARs for agent
    depends_on:
      minio:
        condition: service_healthy
      kafka:
        condition: service_healthy
      postgres:
        condition: service_healthy
      mongodb:
        condition: service_healthy # Dependency on MongoDB for mongo_processor.py
      spline-rest: # IMPORTANT NOTE: Dependency on spline-rest for Spline integration
        condition: service_healthy
    healthcheck: # Added healthcheck
      test: ["CMD-SHELL", "wget -q --spider http://localhost:8080 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5

  spark-worker-1:
    image: bitnami/spark:3.5.0
    container_name: advanced-spark-worker-1
    restart: unless-stopped
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_CORES: 1
      SPARK_WORKER_MEMORY: 1G
      SPARK_EVENT_LOG_ENABLED: "true"
      SPARK_EVENT_LOG_DIR: "/opt/bitnami/spark/events"
      SPARK_SUBMIT_ARGS: > # Workers also need packages for independent execution/driver
        --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,io.delta:delta-core_2.12:2.4.0,org.postgresql:postgresql:42.6.0,org.mongodb.spark:mongo-spark-connector_2.12:10.0.0
        --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
        --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog
        --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000
        --conf spark.hadoop.fs.s3a.access.key=minioadmin
        --conf spark.hadoop.fs.s3a.secret.key=minioadmin
        --conf spark.hadoop.fs.s3a.path.style.access=true
        --conf spark.databricks.delta.properties.defaults.enableChangeDataFeed=true
    volumes:
      - ./data/spark-events:/opt/bitnami/spark/events
      - spline_jars:/opt/bitnami/spark/jars # Mount Spline JARs
    depends_on:
      spark-master:
        condition: service_healthy
      kafka:
        condition: service_healthy
      minio:
        condition: service_healthy
      postgres:
        condition: service_healthy
      mongodb:
        condition: service_healthy
      spline-rest: # IMPORTANT NOTE: Dependency on spline-rest for Spline integration
        condition: service_healthy

  spark-history-server: # Added Spark History Server
    image: bitnami/spark:3.5.0
    container_name: advanced-spark-history-server
    restart: unless-stopped
    command: /opt/bitnami/spark/bin/spark-class org.apache.spark.deploy.history.HistoryServer
    ports:
      - "18080:18080" # Spark History Server UI
    environment:
      SPARK_HISTORY_OPTS: "-Dspark.history.fs.logDirectory=/opt/bitnami/spark/events -Dspark.history.ui.port=18080"
    volumes:
      - ./data/spark-events:/opt/bitnami/spark/events
    depends_on:
      spark-master: # Depends on master to ensure event logs are generated
        condition: service_healthy
    healthcheck: # Added healthcheck
      test: ["CMD", "curl", "-f", "http://localhost:18080"]
      interval: 10s
      timeout: 5s
      retries: 5

  # --- Airflow Services ---
  airflow-init: # Added Airflow Init service to run db upgrade and create user
    image: apache/airflow:2.8.1
    container_name: advanced-airflow-init
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      AIRFLOW_HOME: /opt/airflow
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://user:password@postgres/starter_db
      AIRFLOW__WEBSERVER__WEB_SERVER_PORT: 8080
      _PIP_ADDITIONAL_REQUIREMENTS: "apache-airflow-providers-cncf-kubernetes,apache-airflow-providers-docker,psycopg2-binary" # Add psycopg2 for Postgres
    volumes:
      - ./airflow_dags:/opt/airflow/dags # Mount DAGs for Airflow to discover
    entrypoint: ["/bin/bash", "-cx", "airflow db upgrade && airflow users create --username admin --password admin --firstname Airflow --lastname Admin --role Admin --email admin@example.com || true"]
    healthcheck: # Added healthcheck for init container
      test: ["CMD", "airflow", "version"] # Simple check that airflow cli works
      interval: 5s
      timeout: 5s
      retries: 3
      start_period: 10s # Give it time to start up

  airflow-scheduler:
    image: apache/airflow:2.8.1
    container_name: advanced-airflow-scheduler
    restart: always
    depends_on:
      airflow-init: # Ensure init completes before scheduler starts
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      kafka:
        condition: service_healthy
      spark-master: # Dependency on Spark for running jobs
        condition: service_healthy
      minio: # Dependency for S3/MinIO operations
        condition: service_healthy
      spline-rest: # Dependency for Spline Lineage tasks
        condition: service_healthy
      openmetadata-ingestion: # Dependency for OpenMetadata ingestion tasks
        condition: service_healthy
    environment:
      AIRFLOW_HOME: /opt/airflow
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://user:password@postgres/starter_db
      AIRFLOW__WEBSERVER__WEB_SERVER_PORT: 8080
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      _PIP_ADDITIONAL_REQUIREMENTS: "apache-airflow-providers-cncf-kubernetes,apache-airflow-providers-docker,psycopg2-binary"
    volumes:
      - ./airflow_dags:/opt/airflow/dags
      - ./data/airflow_logs:/opt/airflow/logs
      - ./orchestrator/plugins:/opt/airflow/plugins
      - ./pyspark_jobs:/opt/bitnami/spark/jobs:ro # Read-only mount for Spark jobs from scheduler context
    command: scheduler
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname $$HOSTNAME"]
      interval: 10s
      timeout: 10s
      retries: 5

  airflow-webserver:
    image: apache/airflow:2.8.1
    container_name: advanced-airflow-webserver
    restart: always
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
    ports:
      - "8081:8080" # Mapped to 8081 to avoid conflict with Spark Master UI (8088 now)
    environment:
      AIRFLOW_HOME: /opt/airflow
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://user:password@postgres/starter_db
      AIRFLOW__WEBSERVER__WEB_SERVER_PORT: 8080
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      _PIP_ADDITIONAL_REQUIREMENTS: "apache-airflow-providers-cncf-kubernetes,apache-airflow-providers-docker,psycopg2-binary"
    volumes:
      - ./airflow_dags:/opt/airflow/dags
      - ./data/airflow_logs:/opt/airflow/logs
      - ./orchestrator/plugins:/opt/airflow/plugins
    command: webserver
    healthcheck:
      test: ["CMD-SHELL", "curl --silent --fail http://localhost:8080/health"]
      interval: 10s
      timeout: 10s
      retries: 5

  # --- Observability Components ---
  grafana:
    image: grafana/grafana:latest
    container_name: advanced-grafana
    restart: unless-stopped
    ports:
      - "3000:3000" # Grafana Web UI
    volumes:
      - ./data/grafana:/var/lib/grafana # Persistent storage for Grafana data
      - ./observability/grafana_dashboards:/etc/grafana/provisioning/dashboards # Mount dashboards
      - ./observability/grafana_datasources:/etc/grafana/provisioning/datasources # Mount datasources
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: admin
    depends_on:
      grafana_alloy:
        condition: service_healthy # Changed to service_healthy
      cadvisor:
        condition: service_healthy # Changed to service_healthy
    healthcheck: # Added healthcheck
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 10s
      timeout: 5s
      retries: 5

  grafana_alloy:
    image: grafana/alloy:latest
    container_name: advanced-grafana_alloy
    restart: unless-stopped
    ports:
      - "4317:4317" # OTLP gRPC endpoint for receiving telemetry
      - "4318:4318" # OTLP HTTP endpoint for receiving telemetry
      - "12345:12345" # Example Prometheus scrape port for Grafana to pull metrics from Alloy
    volumes:
      - ./observability/alloy-config.river:/etc/alloy/config.river # Mount your Alloy configuration
    command: -config.file=/etc/alloy/config.river
    healthcheck: # Added healthcheck
      test: ["CMD", "wget", "-qO-", "http://localhost:12345/metrics"]
      interval: 10s
      timeout: 5s
      retries: 5

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.47.0 # Stable version for container metrics
    container_name: advanced-cadvisor
    restart: unless-stopped
    ports:
      - "8082:8080" # Default cAdvisor UI/metrics port (mapped to 8082 to avoid conflicts)
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:rw
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    command: --listen_ip=0.0.0.0 --port=8080 # Expose on all interfaces on port 8080
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:8080/metrics || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # --- Data Lineage (Spline) Components ---
  spline-rest:
    # Using specific versions based on Spline documentation recommendation for stability
    image: absaspline/spline:0.7.1 # Spline REST server and agent bundle
    container_name: advanced-spline-rest
    restart: unless-stopped
    # Exposing 8080 as internal communication, and 8083 for external/debugging
    # OpenMetadata will connect to spline-rest:8080 internally.
    ports:
      - "8083:8080" # Spline REST API server (mapped to 8083 for external access)
    environment:
      SPLINE_DATABASE_CONNECTION_URL: jdbc:h2:mem:spline;DB_CLOSE_DELAY=-1 # Or persistent DB like Postgres
      SPLINE_DATABASE_DRIVER: org.h2.Driver
      SPLINE_DATABASE_USER: sa
      SPLINE_DATABASE_PASSWORD: ""
      # IMPORTANT NOTE: If you want to use Postgres for Spline metadata (recommended for persistence beyond demo):
      # Uncomment the following lines and ensure the `postgres` service is healthy.
      # SPLINE_DATABASE_CONNECTION_URL: jdbc:postgresql://postgres:5432/starter_db
      # SPLINE_DATABASE_DRIVER: org.postgresql.Driver
      # SPLINE_DATABASE_USER: user
      # SPLINE_DATABASE_PASSWORD: password
    depends_on:
      # If using Postgres as Spline DB:
      # postgres:
      #   condition: service_healthy
      # Otherwise, H2 is in-memory
      - postgres # Keeping dependency for overall health
    healthcheck: # Added healthcheck
      test: ["CMD", "curl", "-f", "http://localhost:8080/status || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5

  spline-ui:
    image: absaspline/spline-web-ui:0.7.1 # Matching version with spline-rest
    container_name: advanced-spline-ui
    restart: unless-stopped
    ports:
      - "9090:80" # Spline Web UI
    environment:
      SPLINE_API_URL: http://spline-rest:8080 # Connects to the spline-rest service
    depends_on:
      spline-rest:
        condition: service_healthy
    healthcheck: # Added healthcheck
      test: ["CMD", "curl", "-f", "http://localhost/"]
      interval: 10s
      timeout: 5s
      retries: 5

  # --- Metadata Management (OpenMetadata) Components ---
  openmetadata-mysql:
    image: mysql:8.0
    container_name: advanced-openmetadata-mysql
    restart: unless-stopped
    environment:
      MYSQL_ROOT_PASSWORD: openmetadata_user # Root password for init
      MYSQL_USER: openmetadata_user
      MYSQL_PASSWORD: openmetadata_password
      MYSQL_DATABASE: openmetadata_db
    volumes:
      - ./data/openmetadata_mysql:/var/lib/mysql
    ports:
      - "3306:3306"
    command: --default-authentication-plugin=mysql_native_password
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost", "-u$$MYSQL_USER", "-p$$MYSQL_PASSWORD"]
      interval: 10s
      timeout: 5s
      retries: 5

  openmetadata-elasticsearch:
    image: opensearchproject/opensearch:2.11.0 # Or elasticsearch:7.17.10
    container_name: advanced-openmetadata-elasticsearch
    restart: unless-stopped
    environment:
      discovery.type: single-node
      OPENSEARCH_JAVA_OPTS: "-Xms512m -Xmx512m"
    ports:
      - "9200:9200" # HTTP API
      - "9600:9600" # Transport port
    volumes:
      - ./data/openmetadata_elasticsearch:/usr/share/opensearch/data
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cat/health?h=st | grep -q green"]
      interval: 10s
      timeout: 10s
      retries: 5

  openmetadata-server:
    image: openmetadata/openmetadata:1.3.1
    container_name: advanced-openmetadata-server
    restart: unless-stopped
    depends_on:
      openmetadata-mysql:
        condition: service_healthy
      openmetadata-elasticsearch:
        condition: service_healthy
    ports:
      - "8585:8585" # OpenMetadata Web UI
    environment:
      MYSQL_HOST: openmetadata-mysql
      MYSQL_PORT: 3306
      MYSQL_DATABASE: openmetadata_db
      MYSQL_USER: openmetadata_user
      MYSQL_PASSWORD: openmetadata_password
      ELASTICSEARCH_HOST: openmetadata-elasticsearch
      ELASTICSEARCH_PORT: 9200
      APP_ENV: local
      # Enable OpenMetadata UI features
      # Set to 'true' to allow users to sign up via UI (for local testing)
      # AUTH_TYPE: basic
      # ALLOW_EMAIL_SIGNUP: true
      ENABLE_AUTH: false # IMPORTANT NOTE: For local dev, can disable auth initially for easier access.
    command: ["./docker/run_server.sh"]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8585/api/v1/health | grep -q OK"]
      interval: 30s
      timeout: 20s
      retries: 5

  openmetadata-ingestion:
    image: openmetadata/ingestion-base:1.3.1
    container_name: advanced-openmetadata-ingestion
    restart: on-failure
    depends_on:
      openmetadata-server:
        condition: service_healthy
      kafka: # Dependency for Kafka connector
        condition: service_healthy
      minio: # Dependency for S3 connector
        condition: service_healthy
      postgres: # Dependency for Postgres connector
        condition: service_healthy
      spline-rest: # Dependency for Spline connector
        condition: service_healthy
      fastapi_ingestor: # Dependency for REST API connector
        condition: service_healthy
      mongodb: # Dependency for MongoDB connector
        condition: service_healthy
    environment:
      OPENMETADATA_SERVER_URL: http://openmetadata-server:8585
      AIRFLOW_HOME: /opt/airflow # Required for OpenMetadata to pick up Airflow DAGs if using Airflow connector
    volumes:
      # Mount your custom ingestion scripts and config files
      - ./openmetadata_ingestion_scripts:/opt/openmetadata/examples/workflows
      # Also mount Airflow DAGs if ingestion is part of Airflow context,
      # though generally ingestion scripts run independent of Airflow DAG definitions.
      # - ./airflow_dags:/opt/airflow/dags

  # --- MinIO Event Webhook Listener ---
  webhook_listener:
    build: ./webhook_listener_app # Build from local Dockerfile in webhook_listener_app directory
    container_name: advanced-webhook-listener
    restart: unless-stopped
    ports:
      - "8081:8081" # Expose webhook listener port to host
    depends_on:
      minio:
        condition: service_healthy
    healthcheck: # Added healthcheck
      test: ["CMD", "curl", "-f", "http://localhost:8081/health"] # Assumes /health endpoint exists in Flask app
      interval: 10s
      timeout: 5s
      retries: 3
    # IMPORTANT NOTE: For `webhook_listener` to receive events from MinIO,
    # you must configure MinIO event notifications after MinIO starts.
    # This involves running `mc` commands manually or via an entrypoint script
    # for the `minio` service. See HOWTO guide for details.

# --- Docker Volumes ---
volumes:
  # Volume to persist Spark event logs for the history server
  spark-events:
    driver: local
  # Volume to persist Grafana data
  grafana:
    driver: local
  # IMPORTANT NOTE: You need to manually download Spline agent JARs (spline-spark-agent-bundle_2.12-0.7.1.jar
  # and spline-agent-bundle-0.7.1.jar) from Spline GitHub releases or Maven Central
  # and place them in a local directory (e.g., `./data/spline_jars`).
  # Then, ensure this volume maps that local directory into the Spark containers.
  spline_jars:
    driver: local
  # Volumes for OpenMetadata's databases
  openmetadata_mysql:
    driver: local
  openmetadata_elasticsearch:
    driver: local
  # Volumes for core services data
  starter-postgres:
    driver: local
  starter-minio:
    driver: local
  starter-mongodb:
    driver: local
  airflow_dags:
    driver: local
  airflow_logs:
    driver: local
